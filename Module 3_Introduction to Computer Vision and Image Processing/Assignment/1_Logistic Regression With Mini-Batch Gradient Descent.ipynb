{"cells":[{"cell_type":"markdown","id":"b7386faf-87ec-4cf0-8e6c-9b4f10909cf5","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","id":"478dddc4-19bd-4ebb-8e74-abecfaf5304a","metadata":{},"outputs":[],"source":["<h1>Logistic Regression With Mini-Batch Gradient Descent </h1> \n"]},{"cell_type":"markdown","id":"c2589a34-595c-4f75-9aac-5199ba24a19e","metadata":{},"outputs":[],"source":["<h2>Objective</h2>\n","\n","<ul>\n","    <li>Represent your data as a Dataset object</li>\n","    <li>Create a Logistic Regression Model using PyTorch</li>\n","    <li>Set a Criterion to calculate Loss</li>\n","    <li>Create a Data Loader and set the Batch Size</li>\n","    <li>Create an Optimizer to update Model Parameters and set Learning Rate</li>\n","    <li>Train a Model</li>\n","</ul> \n"]},{"cell_type":"markdown","id":"b0a6cb03-7dba-4ac6-8b57-f5d35062afe9","metadata":{},"outputs":[],"source":["<h2>Table of Contents</h2>\n","<p>In this lab, you will learn how to train a PyTorch Logistic Regression model using Mini-Batch Gradient Descent.</p>\n","\n","<ul>\n","    <li>Load Data</li>\n","    <li>Create the Model and Total Loss Function (Cost)</li>\n","    <li>Setting the Batch Size using a Data Loader</li>\n","    <li>Setting the Learning Rate</li>\n","    <li>Train the Model via Mini-Batch Gradient Descent</li>\n","    <li>Question</li>\n","</ul>\n","<p>Estimated Time Needed: <strong>30 min</strong></p>\n","\n","<hr>\n"]},{"cell_type":"markdown","id":"dd45c4ab-fb0a-42d0-ac77-dab54ca92ecd","metadata":{},"outputs":[],"source":["<h2>Preparation</h2>\n"]},{"cell_type":"markdown","id":"93ea277a-3e98-4fbd-b0c2-ef253cf00217","metadata":{},"outputs":[],"source":["We'll need the following libraries:\n"]},{"cell_type":"code","id":"af203dbf-c26f-4617-87e6-70ea4d3798cd","metadata":{},"outputs":[],"source":["!pip3 install torch torchvision torchaudio"]},{"cell_type":"code","id":"fc9b3990-3a16-4fb3-98d2-4c952901614a","metadata":{},"outputs":[],"source":["# Import the libraries we need for this lab\n\n# Allows us to use arrays to manipulate and store data\nimport numpy as np\n# Used to graph data and loss curves\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits import mplot3d\n# PyTorch Library\nimport torch\n# Used to help create the dataset and perform mini-batch\nfrom torch.utils.data import Dataset, DataLoader\n# PyTorch Neural Network\nimport torch.nn as nn"]},{"cell_type":"markdown","id":"e9047f9d-9910-4092-be4c-a1c9cf35ba17","metadata":{},"outputs":[],"source":["The class <code>plot_error_surfaces</code> is just to help you visualize the data space and the parameter space during training and has nothing to do with Pytorch. \n"]},{"cell_type":"code","id":"bc4dbdb9-82a2-41dd-b571-d65c5c000ff9","metadata":{},"outputs":[],"source":["# Create class for plotting and the function for plotting\n\nclass plot_error_surfaces(object):\n    \n    # Construstor\n    def __init__(self, w_range, b_range, X, Y, n_samples = 30, go = True):\n        W = np.linspace(-w_range, w_range, n_samples)\n        B = np.linspace(-b_range, b_range, n_samples)\n        w, b = np.meshgrid(W, B)    \n        Z = np.zeros((30, 30))\n        count1 = 0\n        self.y = Y.numpy()\n        self.x = X.numpy()\n        for w1, b1 in zip(w, b):\n            count2 = 0\n            for w2, b2 in zip(w1, b1):\n                yhat= 1 / (1 + np.exp(-1*(w2*self.x+b2)))\n                Z[count1,count2]=-1*np.mean(self.y*np.log(yhat+1e-16) +(1-self.y)*np.log(1-yhat+1e-16))\n                count2 += 1   \n            count1 += 1\n        self.Z = Z\n        self.w = w\n        self.b = b\n        self.W = []\n        self.B = []\n        self.LOSS = []\n        self.n = 0\n        if go == True:\n            plt.figure()\n            plt.figure(figsize=(7.5, 5))\n            plt.axes(projection='3d').plot_surface(self.w, self.b, self.Z, rstride=1, cstride=1, cmap='viridis', edgecolor='none')\n            plt.title('Loss Surface')\n            plt.xlabel('w')\n            plt.ylabel('b')\n            plt.show()\n            plt.figure()\n            plt.title('Loss Surface Contour')\n            plt.xlabel('w')\n            plt.ylabel('b')\n            plt.contour(self.w, self.b, self.Z)\n            plt.show()\n            \n     # Setter\n    def set_para_loss(self, model, loss):\n        self.n = self.n + 1\n        self.W.append(list(model.parameters())[0].item())\n        self.B.append(list(model.parameters())[1].item())\n        self.LOSS.append(loss)\n    \n    # Plot diagram\n    def final_plot(self): \n        ax = plt.axes(projection='3d')\n        ax.plot_wireframe(self.w, self.b, self.Z)\n        ax.scatter(self.W, self.B, self.LOSS, c='r', marker='x', s=200, alpha=1)\n        plt.figure()\n        plt.contour(self.w, self.b, self.Z)\n        plt.scatter(self.W, self.B, c='r', marker='x')\n        plt.xlabel('w')\n        plt.ylabel('b')\n        plt.show()\n        \n    # Plot diagram\n    def plot_ps(self):\n        plt.subplot(121)\n        plt.ylim\n        plt.plot(self.x[self.y==0], self.y[self.y==0], 'ro', label=\"training points\")\n        plt.plot(self.x[self.y==1], self.y[self.y==1]-1, 'o', label=\"training points\")\n        plt.plot(self.x, self.W[-1] * self.x + self.B[-1], label=\"estimated line\")\n        plt.xlabel('x')\n        plt.ylabel('y')\n        plt.ylim((-0.1, 2))\n        plt.title('Data Space Iteration: ' + str(self.n))\n        plt.show()\n        plt.subplot(122)\n        plt.contour(self.w, self.b, self.Z)\n        plt.scatter(self.W, self.B, c='r', marker='x')\n        plt.title('Loss Surface Contour Iteration' + str(self.n))\n        plt.xlabel('w')\n        plt.ylabel('b')\n        \n# Plot the diagram\n\ndef PlotStuff(X, Y, model, epoch, leg=True):\n    \n    plt.plot(X.numpy(), model(X).detach().numpy(), label=('epoch ' + str(epoch)))\n    plt.plot(X.numpy(), Y.numpy(), 'r')\n    if leg == True:\n        plt.legend()\n    else:\n        pass"]},{"cell_type":"markdown","id":"9ccc8135-16ae-4070-8d4c-8c2a2a5e227a","metadata":{},"outputs":[],"source":["Set the random seed:\n"]},{"cell_type":"code","id":"fde3193e-f109-4b79-a372-0aae75f96bd4","metadata":{},"outputs":[],"source":["# Setting the seed will allow us to control randomness and give us reproducibility\ntorch.manual_seed(0)"]},{"cell_type":"markdown","id":"ed24f937-1607-440c-9cf2-0681e59ba2d1","metadata":{},"outputs":[],"source":["<!--Empty Space for separating topics-->\n"]},{"cell_type":"markdown","id":"2308f3c2-4239-4c98-b4a1-df559ffd3dc7","metadata":{},"outputs":[],"source":["<h2 id=\"Makeup_Data\">Load Data</h2>\n"]},{"cell_type":"markdown","id":"10cf5cb5-e867-4837-b00c-0a6544d3ab6a","metadata":{},"outputs":[],"source":["The Dataset class represents a dataset. Your custom dataset should inherit Dataset which we imported above and override the following methods:\n","\n","<p><code>__len__</code> so that len(dataset) returns the size of the dataset.</p>\n","<p><code>__getitem__</code> to support the indexing such that dataset[i] can be used to get ith sample</p>\n","\n","Below we will create a sample dataset\n"]},{"cell_type":"code","id":"df7e6857-7ec5-4f20-8907-0f914e92ec22","metadata":{},"outputs":[],"source":["# Create the custom Data class which inherits Dataset\nclass Data(Dataset):\n    \n    # Constructor\n    def __init__(self):\n        # Create X values from -1 to 1 with step .1\n        self.x = torch.arange(-1, 1, 0.1).view(-1, 1)\n        # Create Y values all set to 0\n        self.y = torch.zeros(self.x.shape[0], 1)\n        # Set the X values above 0.2 to 1\n        self.y[self.x[:, 0] > 0.2] = 1\n        # Set the .len attribute because we need to override the __len__ method\n        self.len = self.x.shape[0]\n    \n    # Getter that returns the data at the given index\n    def __getitem__(self, index):      \n        return self.x[index], self.y[index]\n    \n    # Get length of the dataset\n    def __len__(self):\n        return self.len"]},{"cell_type":"markdown","id":"a190e4c9-db41-4a9e-956a-e1da8f557ff9","metadata":{},"outputs":[],"source":["Make <code>Data</code> object\n"]},{"cell_type":"code","id":"f7b069cb-9fb6-4316-b329-33cfbb2a0988","metadata":{},"outputs":[],"source":["# Create Data object\ndata_set = Data()"]},{"cell_type":"markdown","id":"bb631e2a-bdd4-492a-991c-337e0dde6744","metadata":{},"outputs":[],"source":["We can see the X values of the dataset\n"]},{"cell_type":"code","id":"b9d655c4-39cc-4a0c-9406-1a800021dbee","metadata":{},"outputs":[],"source":["data_set.x"]},{"cell_type":"markdown","id":"89561ca3-5ce9-48d6-a896-eb1bd2a87cb9","metadata":{},"outputs":[],"source":["We can see the Y values of the dataset which correspond to the class of the X value\n"]},{"cell_type":"code","id":"2fd6eb7d-bdda-4d62-80de-2d3a0a2c35b1","metadata":{},"outputs":[],"source":["data_set.y"]},{"cell_type":"markdown","id":"9df63984-b00b-4472-8195-68dd042d9357","metadata":{},"outputs":[],"source":["We can get the length of the dataset\n"]},{"cell_type":"code","id":"6be74b4c-f798-404f-9cb1-523f8a3cfb7d","metadata":{},"outputs":[],"source":["len(data_set)"]},{"cell_type":"markdown","id":"a6e856a9-e2f1-4110-b3d2-55b31460f3be","metadata":{},"outputs":[],"source":["We can get the label $y$ as well as the $x$ for the first sample \n"]},{"cell_type":"code","id":"845d9091-3eb3-49f7-8184-2cbf53b0072d","metadata":{},"outputs":[],"source":["x,y = data_set[0]\nprint(\"x = {},  y = {}\".format(x,y))"]},{"cell_type":"markdown","id":"560b7060-5a83-4219-8ecc-1fd0d09daf4b","metadata":{},"outputs":[],"source":["We can get the label $y$ as well as the $x$ for the second sample:\n"]},{"cell_type":"code","id":"05a7da85-917c-456c-b731-80515d271819","metadata":{},"outputs":[],"source":["x,y = data_set[1]\nprint(\"x = {},  y = {}\".format(x,y))"]},{"cell_type":"markdown","id":"13fe0955-50ab-4b4e-9e5e-2419d6398ecb","metadata":{},"outputs":[],"source":[" We can see we can separate the one-dimensional dataset into two classes:\n"]},{"cell_type":"code","id":"625102af-fa34-4cc1-aad2-6e309d7871b2","metadata":{},"outputs":[],"source":["plt.plot(data_set.x[data_set.y==0], data_set.y[data_set.y==0], 'ro', label=\"y=0\")\nplt.plot(data_set.x[data_set.y==1], data_set.y[data_set.y==1]-1, 'o', label=\"y=1\")\nplt.xlabel('x')\nplt.legend()          "]},{"cell_type":"markdown","id":"c907d56a-649d-4c4b-9d00-1435ee51f0f8","metadata":{},"outputs":[],"source":["<!--Empty Space for separating topics-->\n"]},{"cell_type":"markdown","id":"4634d009-388c-44fa-a5ce-52ce0e712c04","metadata":{},"outputs":[],"source":["<h2 id=\"Model_Cost\">Create the Model and Total Loss Function (Cost)</h2>\n"]},{"cell_type":"markdown","id":"e1900240-f557-4fd6-98f0-93ccfb5a9971","metadata":{},"outputs":[],"source":["For Logistic Regression typically we would not use PyTorch instead we would use Scikit-Learn as it is easier to use and set up. We are using PyTorch because it is good practice for deep learning. Scikit-Learn is typically used for Machine Learning while PyTorch is used for Deep Learning.\n"]},{"cell_type":"markdown","id":"72614bf3-1d81-4fee-93fc-4e7989ebec72","metadata":{},"outputs":[],"source":["We will create a custom class that defines the architecture of Logistic Regression using PyTorch. Logistic Regression has a single layer where the input is the number of features an X value of the dataset has (dimension of X) and there is a single output. The output of the layer is put into a sigmoid function which is a function between 0 and 1. The larger the output of the layer the closer it is to 1 and the smaller the output is the closer it is to 0. The sigmoid function will allow us to turn this output into a classification problem. If the output value is closer to 1 it is one class if it is closer to 0 it is in another.\n"]},{"cell_type":"markdown","id":"9f3d7fbb-a3f7-4108-8f56-25dd2e0ee02b","metadata":{},"outputs":[],"source":["Sigmoid Function\n","\n","![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-Coursera/labs/Module3/Logistic-curve.svg)\n"]},{"cell_type":"code","id":"1a9cfb60-380a-4da5-828b-d12244320876","metadata":{},"outputs":[],"source":["# Create logistic_regression class that inherits nn.Module which is the base class for all neural networks\nclass logistic_regression(nn.Module):\n    \n    # Constructor\n    def __init__(self, n_inputs):\n        super(logistic_regression, self).__init__()\n        # Single layer of Logistic Regression with number of inputs being n_inputs and there being 1 output \n        self.linear = nn.Linear(n_inputs, 1)\n        \n    # Prediction\n    def forward(self, x):\n        # Using the input x value puts it through the single layer defined above then puts the output through the sigmoid function and returns the result\n        yhat = torch.sigmoid(self.linear(x))\n        return yhat"]},{"cell_type":"markdown","id":"f4a81b06-aed7-45aa-be7b-78432203aca4","metadata":{},"outputs":[],"source":["We can check the number of features an X value has, the size of the input, or the dimension of X\n"]},{"cell_type":"code","id":"dd9ea5f7-03de-4337-93dd-5fafed6ee52e","metadata":{},"outputs":[],"source":["x,y = data_set[0]\nlen(x)"]},{"cell_type":"markdown","id":"81c2cf2a-53a4-4d9d-97b0-f5b010acd8d1","metadata":{},"outputs":[],"source":["Create a logistic regression object or model, the input parameter is the number of dimensions.\n"]},{"cell_type":"code","id":"d8cf5e81-4844-4e35-8428-e985a2681e4c","metadata":{},"outputs":[],"source":["# Create the logistic_regression result\n\nmodel = logistic_regression(1)"]},{"cell_type":"markdown","id":"386756b5-1647-486f-9b3b-5f29fa984bc9","metadata":{},"outputs":[],"source":["We can make a prediction sigma $\\sigma$ this uses the forward function defined above\n"]},{"cell_type":"code","id":"8bf47223-83b9-4666-9170-c6e760fa43cf","metadata":{},"outputs":[],"source":["x = torch.tensor([-1.0])\n\nsigma = model(x)\nsigma"]},{"cell_type":"markdown","id":"9c293024-e024-4617-8999-98754b8cf6a4","metadata":{},"outputs":[],"source":["We can also make a prediction using our data\n"]},{"cell_type":"code","id":"001ae5a1-edee-4b12-a833-9b091d8f96a2","metadata":{},"outputs":[],"source":["x,y = data_set[2]\n\nsigma = model(x)\nsigma"]},{"cell_type":"markdown","id":"ac5da279-e3bb-4a23-bdc9-0a4bc3c3764e","metadata":{},"outputs":[],"source":["Create a <code>plot_error_surfaces</code> object to visualize the data space and the learnable parameters space during training:\n","\n","We can see on the Loss Surface graph, the loss value varying across w and b values with yellow being high loss and dark blue being low loss which is what we want\n","\n","On the Loss Surface Contour graph we can see a top-down view of the Loss Surface graph\n"]},{"cell_type":"code","id":"df91482f-0f08-46a4-9ff0-6ce2b071aecf","metadata":{},"outputs":[],"source":["# Create the plot_error_surfaces object\n\n# 15 is the range of w\n# 13 is the range of b\n# data_set[:][0] are all the X values\n# data_set[:][1] are all the Y values\n\nget_surface = plot_error_surfaces(15, 13, data_set[:][0], data_set[:][1])"]},{"cell_type":"markdown","id":"16c1cba8-da86-4603-8810-70de464b4c8d","metadata":{},"outputs":[],"source":["We define a criterion using Binary Cross Entropy Loss. This will measure the difference/loss between the prediction and actual value.\n"]},{"cell_type":"code","id":"5cd5787b-8716-4772-a024-4f88ecc2ec66","metadata":{},"outputs":[],"source":["criterion = nn.BCELoss()"]},{"cell_type":"markdown","id":"9076dcee-e07f-4b03-9529-9fc6b974cf23","metadata":{},"outputs":[],"source":["We have our samples:\n"]},{"cell_type":"code","id":"6dd0bd99-34c6-4531-b57f-0bde7151dfb1","metadata":{},"outputs":[],"source":["x, y = data_set[0]\nprint(\"x = {},  y = {}\".format(x,y))"]},{"cell_type":"markdown","id":"388f3ed4-e069-4cf2-9190-394f1dfb9074","metadata":{},"outputs":[],"source":["We can make a prediction using the model:\n"]},{"cell_type":"code","id":"0fdf4b78-d906-45cd-80c1-bbc8137bc0e9","metadata":{},"outputs":[],"source":["sigma = model(x)\nsigma"]},{"cell_type":"markdown","id":"e4f4070b-3a76-48a6-b578-d38170e2ee3b","metadata":{},"outputs":[],"source":["We can calculate the loss \n"]},{"cell_type":"code","id":"4dee3ccb-f2f6-41c6-89f6-de0666923027","metadata":{},"outputs":[],"source":["loss = criterion(sigma, y)\nloss"]},{"cell_type":"markdown","id":"0b14f34d-b8fb-47aa-a8fc-0651126e9067","metadata":{},"outputs":[],"source":["## Setting the Batch Size using a Data Loader\n"]},{"cell_type":"markdown","id":"26bdd7a1-161c-4603-bb47-9cf331f9f104","metadata":{},"outputs":[],"source":["You have to use data loader in PyTorch that will output a  batch of data, the input is the <code>dataset</code> and  <code>batch_size</code>\n"]},{"cell_type":"code","id":"3d95dd2f-e6ff-47ac-8c70-012ad59cedea","metadata":{},"outputs":[],"source":["batch_size=10"]},{"cell_type":"code","id":"d845bee6-909f-412e-874e-b545f0914918","metadata":{},"outputs":[],"source":["trainloader = DataLoader(dataset = data_set, batch_size = 10)"]},{"cell_type":"code","id":"df3c67d9-ec74-4e53-9e9e-f53f86741604","metadata":{},"outputs":[],"source":["dataset_iter = iter(trainloader)"]},{"cell_type":"code","id":"e90883bb-265d-4aa3-bacc-62c1e14e8cfe","metadata":{},"outputs":[],"source":["X,y=next(dataset_iter )"]},{"cell_type":"markdown","id":"1159b894-2895-45ad-8985-904447f3fd52","metadata":{},"outputs":[],"source":["We can see here that 10 values the same as our batch size\n"]},{"cell_type":"code","id":"c903c356-8cd7-4acb-8fce-003b37f5ad1f","metadata":{},"outputs":[],"source":["X"]},{"cell_type":"markdown","id":"d9099e8a-4693-4d55-8926-cb831df6a162","metadata":{},"outputs":[],"source":["## Setting the Learning Rate\n"]},{"cell_type":"markdown","id":"a1cf7dd0-5301-4211-86b2-a09c77cd5826","metadata":{},"outputs":[],"source":["We can set the learning rate by setting it as a parameter in the optimizer along with the parameters of the logistic regression model we are training. The job of the optimizer, torch.optim.SGD, is to use the loss generated by the criterion to update the model parameters according to the learning rate. SGD stands for Stochastic Gradient Descent which typically means that the batch size is set to 1, but the data loader we set up above has turned this into Mini-Batch Gradient Descent.\n"]},{"cell_type":"code","id":"b03792fb-b4fe-48f6-a9f0-b54cd4e4947e","metadata":{},"outputs":[],"source":["learning_rate = 0.1"]},{"cell_type":"code","id":"d784df5e-892b-4c74-a991-a096dc9f0ae0","metadata":{},"outputs":[],"source":["optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"]},{"cell_type":"markdown","id":"08a9b572-6819-4775-b6ff-2f7f540edf81","metadata":{},"outputs":[],"source":["<!--Empty Space for separating topics-->\n"]},{"cell_type":"markdown","id":"a5142d8c-a918-4850-b5cc-c83fbab1bfaa","metadata":{},"outputs":[],"source":["<h2 id=\"BGD\">Train the Model via Mini-Batch Gradient Descent</h2>\n"]},{"cell_type":"markdown","id":"1d6ca125-82c9-433e-8e64-fbb927a1674c","metadata":{},"outputs":[],"source":["We are going to train the model using various Batch Sizes and Learning Rates.\n"]},{"cell_type":"markdown","id":"4b0bdf44-08cd-47f5-ab56-5068b0224cda","metadata":{},"outputs":[],"source":["### Mini-Batch Gradient Descent\n"]},{"cell_type":"markdown","id":"5d54d9ab-78d9-4cd0-8541-ee0572245d03","metadata":{},"outputs":[],"source":["In this case, we will set the batch size of the data loader to 5 and the number of epochs to 250.\n"]},{"cell_type":"markdown","id":"51c8f5ad-ed4c-4ee7-801e-ce8c848217c5","metadata":{},"outputs":[],"source":["First, we must recreate the get_surface object again so that for each example we get a Loss Surface for that model only.\n"]},{"cell_type":"code","id":"4df8e800-b96d-4e4c-ab39-0bea36374c23","metadata":{},"outputs":[],"source":["get_surface = plot_error_surfaces(15, 13, data_set[:][0], data_set[:][1], 30)"]},{"cell_type":"markdown","id":"064e732b-ed4c-417a-afc6-618ccfbeff18","metadata":{},"outputs":[],"source":["#### Train the Model\n"]},{"cell_type":"code","id":"f26a8f6e-aa4a-4186-b898-5f38decbee41","metadata":{},"outputs":[],"source":["# First we create an instance of the model we want to train\nmodel = logistic_regression(1)\n# We create a criterion which will measure loss\ncriterion = nn.BCELoss()\n# We create a data loader with the dataset and specified batch size of 5\ntrainloader = DataLoader(dataset = data_set, batch_size = 5)\n# We create an optimizer with the model parameters and learning rate\noptimizer = torch.optim.SGD(model.parameters(), lr = .01)\n# Then we set the number of epochs which is the total number of times we will train on the entire training dataset\nepochs=500\n# This will store the loss over iterations so we can plot it at the end\nloss_values = []\n\n# Loop will execute for number of epochs\nfor epoch in range(epochs):\n    # For each batch in the training data\n    for x, y in trainloader:\n        # Make our predictions from the X values\n        yhat = model(x)\n        # Measure the loss between our prediction and actual Y values\n        loss = criterion(yhat, y)\n        # Resets the calculated gradient value, this must be done each time as it accumulates if we do not reset\n        optimizer.zero_grad()\n        # Calculates the gradient value with respect to each weight and bias\n        loss.backward()\n        # Updates the weight and bias according to calculated gradient value\n        optimizer.step()\n        # Set the parameters for the loss surface contour graphs\n        get_surface.set_para_loss(model, loss.tolist())\n        # Saves the loss of the iteration\n        loss_values.append(loss)\n    # Want to print the Data Space for the current iteration every 20 epochs\n    if epoch % 20 == 0:\n        get_surface.plot_ps()"]},{"cell_type":"markdown","id":"25a2fc79-88f4-4ae5-ae52-e723ec897561","metadata":{},"outputs":[],"source":["We can see the final values of the weight and bias. This weight and bias correspond to the orange line in the Data Space graph and the final spot of the X in the Loss Surface Contour graph.\n"]},{"cell_type":"code","id":"d0655799-aec4-4f1b-b3c6-31c9980e02b2","metadata":{},"outputs":[],"source":["w = model.state_dict()['linear.weight'].data[0]\nb = model.state_dict()['linear.bias'].data[0]\nprint(\"w = \", w, \"b = \", b)"]},{"cell_type":"markdown","id":"6f4f636b-9381-4b34-9f0e-8b608d209ac7","metadata":{},"outputs":[],"source":["Now we can get the accuracy of the training data\n"]},{"cell_type":"code","id":"5ff884d1-9e2f-4d19-8e2f-4daa65f1c251","metadata":{},"outputs":[],"source":["# Getting the predictions\nyhat = model(data_set.x)\n# Rounding the prediction to the nearedt integer 0 or 1 representing the classes\nyhat = torch.round(yhat)\n# Counter to keep track of correct predictions\ncorrect = 0\n# Goes through each prediction and actual y value\nfor prediction, actual in zip(yhat, data_set.y):\n    # Compares if the prediction and actualy y value are the same\n    if (prediction == actual):\n        # Adds to counter if prediction is correct\n        correct+=1\n# Outputs the accuracy by dividing the correct predictions by the length of the dataset\nprint(\"Accuracy: \", correct/len(data_set)*100, \"%\")"]},{"cell_type":"markdown","id":"4876c2de-d169-4cd4-8c12-f86c6ef1ed3f","metadata":{},"outputs":[],"source":["Finally, we plot the Cost vs Iteration graph, although it is erratic it is downward sloping.\n"]},{"cell_type":"code","id":"51d359bc-6de8-4749-a659-c7732f6fb3a1","metadata":{},"outputs":[],"source":["LOSS_BGD1=[]\nfor i in loss_values:\n    LOSS_BGD1.append(i.item())\n\nplt.plot(LOSS_BGD1)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Cost\")\n"]},{"cell_type":"markdown","id":"548b9f8e-6a8a-4336-8551-16417132802c","metadata":{},"outputs":[],"source":["### Stochastic Gradient Descent\n"]},{"cell_type":"markdown","id":"72248cf7-d9a6-4adc-86d1-fb5b39cb4b01","metadata":{},"outputs":[],"source":["In this case, we will set the batch size of the data loder to 1 so that the gradient descent will be performed for each example this is referred to as Stochastic Gradient Descent. The number of epochs is set to 100.\n","\n","Notice that in this example the batch size is decreased from 5 to 1 so there would be more iterations. Due to this, we can reduce the number of iterations by decreasing the number of epochs. Due to the reduced batch size, we are optimizing more frequently so we don't need as many epochs.\n","\n","First, we must recreate the `get_surface` object again so that for each example we get a Loss Surface for that model only.\n"]},{"cell_type":"code","id":"2901faaa-63b0-4c70-a7ff-42375b50c7d5","metadata":{},"outputs":[],"source":["get_surface = plot_error_surfaces(15, 13, data_set[:][0], data_set[:][1], 30)"]},{"cell_type":"markdown","id":"bba63405-17b0-486a-becb-d6efc81895b4","metadata":{},"outputs":[],"source":["#### Train the Model\n"]},{"cell_type":"code","id":"d5a497b1-cd75-42d8-8441-0f7c90b10b0b","metadata":{},"outputs":[],"source":["# First we create an instance of the model we want to train\nmodel = logistic_regression(1)\n# We create a criterion which will measure loss\ncriterion = nn.BCELoss()\n# We create a data loader with the dataset and specified batch size of 1\ntrainloader = DataLoader(dataset = data_set, batch_size = 1)\n# We create an optimizer with the model parameters and learning rate\noptimizer = torch.optim.SGD(model.parameters(), lr = .01)\n# Then we set the number of epochs which is the total number of times we will train on the entire training dataset\nepochs=100\n# This will store the loss over iterations so we can plot it at the end\nloss_values = []\n\n# Loop will execute for number of epochs\nfor epoch in range(epochs):\n    # For each batch in the training data\n    for x, y in trainloader:\n        # Make our predictions from the X values\n        yhat = model(x)\n        # Measure the loss between our prediction and actual Y values\n        loss = criterion(yhat, y)\n        # Resets the calculated gradient value, this must be done each time as it accumulates if we do not reset\n        optimizer.zero_grad()\n        # Calculates the gradient value with respect to each weight and bias\n        loss.backward()\n        # Updates the weight and bias according to calculated gradient value\n        optimizer.step()\n        # Set the parameters for the loss surface contour graphs\n        get_surface.set_para_loss(model, loss.tolist())\n        # Saves the loss of the iteration\n        loss_values.append(loss)\n    # Want to print the Data Space for the current iteration every 20 epochs\n    if epoch % 20 == 0:\n        get_surface.plot_ps()"]},{"cell_type":"markdown","id":"592826d5-1aa6-470b-a42c-64ebf900c2be","metadata":{},"outputs":[],"source":["We can see the final values of the weight and bias. This weight and bias correspond to the orange line in the Data Space graph and the final spot of the X in the Loss Surface Contour graph.\n"]},{"cell_type":"code","id":"0d6ad8d4-6e42-4cbe-917f-fb8b3595d798","metadata":{},"outputs":[],"source":["w = model.state_dict()['linear.weight'].data[0]\nb = model.state_dict()['linear.bias'].data[0]\nprint(\"w = \", w, \"b = \", b)"]},{"cell_type":"markdown","id":"820d013a-9d8e-490a-a2c1-e416889a905c","metadata":{},"outputs":[],"source":["Now we can get the accuracy of the training data\n"]},{"cell_type":"code","id":"e2879d3b-e1e2-4384-a645-8b0ca8442088","metadata":{},"outputs":[],"source":["# Getting the predictions\nyhat = model(data_set.x)\n# Rounding the prediction to the nearedt integer 0 or 1 representing the classes\nyhat = torch.round(yhat)\n# Counter to keep track of correct predictions\ncorrect = 0\n# Goes through each prediction and actual y value\nfor prediction, actual in zip(yhat, data_set.y):\n    # Compares if the prediction and actualy y value are the same\n    if (prediction == actual):\n        # Adds to counter if prediction is correct\n        correct+=1\n# Outputs the accuracy by dividing the correct predictions by the length of the dataset\nprint(\"Accuracy: \", correct/len(data_set)*100, \"%\")"]},{"cell_type":"markdown","id":"6863e47c-c493-472e-a668-009c74fe8781","metadata":{},"outputs":[],"source":["Finally, we plot the Cost vs Iteration graph, although it is erratic it is downward sloping.\n"]},{"cell_type":"code","id":"8032fafc-8752-442a-aa44-0e69db8d1697","metadata":{},"outputs":[],"source":["LOSS_BGD1=[]\nfor i in loss_values:\n    LOSS_BGD1.append(i.item())\n\n \nplt.plot(LOSS_BGD1)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Cost\")\n"]},{"cell_type":"markdown","id":"8ea33232-b1de-4c93-8ea0-29c89fad854f","metadata":{},"outputs":[],"source":["### High Learning Rate\n"]},{"cell_type":"markdown","id":"e44a9326-b046-43f8-99b1-57fce1fd1778","metadata":{},"outputs":[],"source":["In this case, we will set the batch size of the data loder to 1 so that the gradient descent will be performed for each example this is referred to as Stochastic Gradient Descent. This time the learning rate will be set to .1 to represent a high learning rate and we will observe what will happen when we try to train.\n","\n","First, we must recreate the `get_surface` object again so that for each example we get a Loss Surface for that model only.\n"]},{"cell_type":"code","id":"db6e7c6d-ab00-431b-8227-4bc4ce627ce4","metadata":{},"outputs":[],"source":["get_surface = plot_error_surfaces(15, 13, data_set[:][0], data_set[:][1], 30)"]},{"cell_type":"markdown","id":"f45b3d1a-2720-40e7-bbdf-fcecf2f578de","metadata":{},"outputs":[],"source":["#### Train the Model\n"]},{"cell_type":"code","id":"27e47567-e491-4321-acb3-573f649f4657","metadata":{},"outputs":[],"source":["# First we create an instance of the model we want to train\nmodel = logistic_regression(1)\n# We create a criterion that will measure loss\ncriterion = nn.BCELoss()\n# We create a data loader with the dataset and specified batch size of 1\ntrainloader = DataLoader(dataset = data_set, batch_size = 1)\n# We create an optimizer with the model parameters and learning rate\noptimizer = torch.optim.SGD(model.parameters(), lr = 1)\n# Then we set the number of epochs which is the total number of times we will train on the entire training dataset\nepochs=100\n# This will store the loss over iterations so we can plot it at the end\nloss_values = []\n\n# Loop will execute for number of epochs\nfor epoch in range(epochs):\n    # For each batch in the training data\n    for x, y in trainloader:\n        # Make our predictions from the X values\n        yhat = model(x)\n        # Measure the loss between our prediction and actual Y values\n        loss = criterion(yhat, y)\n        # Resets the calculated gradient value, this must be done each time as it accumulates if we do not reset\n        optimizer.zero_grad()\n        # Calculates the gradient value with respect to each weight and bias\n        loss.backward()\n        # Updates the weight and bias according to calculated gradient value\n        optimizer.step()\n        # Set the parameters for the loss surface contour graphs\n        get_surface.set_para_loss(model, loss.tolist())\n        # Saves the loss of the iteration\n        loss_values.append(loss)\n    # Want to print the Data Space for the current iteration every 20 epochs\n    if epoch % 20 == 0:\n        get_surface.plot_ps()"]},{"cell_type":"markdown","id":"0516e83b-5192-45c9-ab44-17030ca2c685","metadata":{},"outputs":[],"source":["Notice in this example the due to the high learning rate the Loss Surface Contour graph has increased movement over the previous example and also moves in multiple directions due to the minimum being overshot.\n"]},{"cell_type":"markdown","id":"4f89b783-8d1c-49da-98fe-600c4a459fb9","metadata":{},"outputs":[],"source":["We can see the final values of the weight and bias. This weight and bias correspond to the orange line in the Data Space graph and the final spot of the X in the Loss Surface Contour graph.\n"]},{"cell_type":"code","id":"0f99080c-5bb3-4a80-a32e-b50adc1ebfcb","metadata":{},"outputs":[],"source":["w = model.state_dict()['linear.weight'].data[0]\nb = model.state_dict()['linear.bias'].data[0]\nprint(\"w = \", w, \"b = \", b)"]},{"cell_type":"markdown","id":"2b3ab408-1eae-4f10-ab0c-f8f8ec90dc36","metadata":{},"outputs":[],"source":["Now we can get the accuracy of the training data\n"]},{"cell_type":"code","id":"4b3af6c8-d097-4145-83e1-2fe811ae86b4","metadata":{},"outputs":[],"source":["# Getting the predictions\nyhat = model(data_set.x)\n# Rounding the prediction to the nearedt integer 0 or 1 representing the classes\nyhat = torch.round(yhat)\n# Counter to keep track of correct predictions\ncorrect = 0\n# Goes through each prediction and actual y value\nfor prediction, actual in zip(yhat, data_set.y):\n    # Compares if the prediction and actualy y value are the same\n    if (prediction == actual):\n        # Adds to counter if prediction is correct\n        correct+=1\n# Outputs the accuracy by dividing the correct predictions by the length of the dataset\nprint(\"Accuracy: \", correct/len(data_set)*100, \"%\")"]},{"cell_type":"markdown","id":"2fe13ccb-ed57-4557-a775-ba7a3ef80fa2","metadata":{},"outputs":[],"source":["Finally, we plot the Cost vs Iteration graph, although it is erratic it is downward sloping.\n"]},{"cell_type":"code","id":"ea39985b-251a-43c7-8634-b1388c06d705","metadata":{},"outputs":[],"source":["LOSS_BGD1=[]\nfor i in loss_values:\n    LOSS_BGD1.append(i.item())\n\n \nplt.plot(LOSS_BGD1)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Cost\")\n"]},{"cell_type":"markdown","id":"738080a5-5b62-4439-a624-b449bc43fb05","metadata":{},"outputs":[],"source":["## Question\n"]},{"cell_type":"markdown","id":"058a50a6-a16d-466b-815c-4c3a0731f610","metadata":{},"outputs":[],"source":["Using the following code train the model using a `learning rate` of `.01`, `120 epochs`, and `batch_size` of `1`.\n"]},{"cell_type":"code","id":"80a6439e-146b-4ded-87e3-30460280bea2","metadata":{},"outputs":[],"source":["get_surface = plot_error_surfaces(15, 13, data_set[:][0], data_set[:][1], 30)"]},{"cell_type":"markdown","id":"c47b08e9-821a-4c9a-8f45-792e2d831145","metadata":{},"outputs":[],"source":["#### Train the Model\n"]},{"cell_type":"code","id":"dc22223a-f989-4496-98aa-065411171a91","metadata":{},"outputs":[],"source":["# First we create an instance of the model we want to train\nmodel = logistic_regression(1)\n# We create a criterion which will measure loss\ncriterion = nn.BCELoss()\n# We create a data loader with the dataset and specified batch size of 1\ntrainloader = DataLoader(dataset = data_set, batch_size = \"SET_BATCH_SIZE\")\n# We create an optimizer with the model parameters and learning rate\noptimizer = torch.optim.SGD(model.parameters(), lr = \"SET_LEARNING_RATE\")\n# Then we set the number of epochs which is the total number of times we will train on the entire training dataset\nepochs= \"SET_NUMBER_OF_EPOCHS\"\n# This will store the loss over iterations so we can plot it at the end\nloss_values = []\n\n# Loop will execute for number of epochs\nfor epoch in range(epochs):\n    # For each batch in the training data\n    for x, y in trainloader:\n        # Make our predictions from the X values\n        yhat = model(x)\n        # Measure the loss between our prediction and actual Y values\n        loss = criterion(yhat, y)\n        # Resets the calculated gradient value, this must be done each time as it accumulates if we do not reset\n        optimizer.zero_grad()\n        # Calculates the gradient value with respect to each weight and bias\n        loss.backward()\n        # Updates the weight and bias according to calculated gradient value\n        optimizer.step()\n        # Set the parameters for the loss surface contour graphs\n        get_surface.set_para_loss(model, loss.tolist())\n        # Saves the loss of the iteration\n        loss_values.append(loss)\n    # Want to print the Data Space for the current iteration every 20 epochs\n    if epoch % 20 == 0:\n        get_surface.plot_ps()"]},{"cell_type":"markdown","id":"126d91e6-64a4-45a3-b1ce-4ff00fd2b9a9","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","<code>  \n","# First we create an instance of the model we want to train\n","model = logistic_regression(1)\n","# We create a criterion which will measure loss\n","criterion = nn.BCELoss()\n","# We create a data loader with the dataset and specified batch size of 1\n","trainloader = DataLoader(dataset = data_set, batch_size = 1)\n","# We create an optimizer with the model parameters and learning rate\n","optimizer = torch.optim.SGD(model.parameters(), lr = .01)\n","# Then we set the number of epochs which is the total number of times we will train on the entire training dataset\n","epochs= 120\n","# This will store the loss over iterations so we can plot it at the end\n","loss_values = []\n","# Loop will execute for number of epochs\n","for epoch in range(epochs):\n","    # For each batch in the training data\n","    for x, y in trainloader:\n","        # Make our predictions from the X values\n","        yhat = model(x)\n","        # Measure the loss between our prediction and actual Y values\n","        loss = criterion(yhat, y)\n","        # Resets the calculated gradient value, this must be done each time as it accumulates if we do not reset\n","        optimizer.zero_grad()\n","        # Calculates the gradient value with respect to each weight and bias\n","        loss.backward()\n","        # Updates the weight and bias according to calculated gradient value\n","        optimizer.step()\n","        # Set the parameters for the loss surface contour graphs\n","        get_surface.set_para_loss(model, loss.tolist())\n","        # Saves the loss of the iteration\n","        loss_values.append(loss)\n","    # Want to print the Data Space for the current iteration every 20 epochs\n","    if epoch % 20 == 0:\n","        get_surface.plot_ps()\n","</code>\n","</details>\n"]},{"cell_type":"markdown","id":"f7f63a1f-641a-44a2-ab7e-7fd19e17fb45","metadata":{},"outputs":[],"source":["We can see the final values of the weight and bias. This weight and bias correspond to the orange line in the Data Space graph and the final spot of the X in the Loss Surface Contour graph.\n"]},{"cell_type":"code","id":"29160646-fa3b-4a73-a4c2-592bf9723591","metadata":{},"outputs":[],"source":["w = model.state_dict()['linear.weight'].data[0]\nb = model.state_dict()['linear.bias'].data[0]\nprint(\"w = \", w, \"b = \", b)"]},{"cell_type":"markdown","id":"3daa1357-70ee-4833-9fc0-615da68441f8","metadata":{},"outputs":[],"source":["Now we can get the accuracy of the training data\n"]},{"cell_type":"code","id":"e6352656-1a66-42c2-a1b5-190badc94dd8","metadata":{},"outputs":[],"source":["# Getting the predictions\nyhat = model(data_set.x)\n# Rounding the prediction to the nearedt integer 0 or 1 representing the classes\nyhat = torch.round(yhat)\n# Counter to keep track of correct predictions\ncorrect = 0\n# Goes through each prediction and actual y value\nfor prediction, actual in zip(yhat, data_set.y):\n    # Compares if the prediction and actualy y value are the same\n    if (prediction == actual):\n        # Adds to counter if prediction is correct\n        correct+=1\n# Outputs the accuracy by dividing the correct predictions by the length of the dataset\nprint(\"Accuracy: \", correct/len(data_set)*100, \"%\")"]},{"cell_type":"markdown","id":"f123d70a-91db-4be1-a05b-7f771dc5e9bd","metadata":{},"outputs":[],"source":["Finally, we plot the Cost vs Iteration graph, although it is erratic it is downward sloping.\n"]},{"cell_type":"code","id":"3175f6ab-47b6-4160-8e02-52333f05b03a","metadata":{},"outputs":[],"source":["LOSS_BGD1=[]\nfor i in loss_values:\n    LOSS_BGD1.append(i.item())\n\n \nplt.plot(LOSS_BGD1)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Cost\")\n"]},{"cell_type":"markdown","id":"3a5a836f-b740-48da-9bd2-8eea2d42e2d7","metadata":{},"outputs":[],"source":["<a href=\"https://dataplatform.cloud.ibm.com/registration/stepone?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-IBMDeveloperSkillsNetwork-CV0101EN-Coursera&context=cpdaas&apps=data_science_experience%2Cwatson_machine_learning\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/Template/module%201/images/Watson_Studio.png\"></a>\n"]},{"cell_type":"markdown","id":"5a810c13-5ee5-46bc-9542-d9d758d256f1","metadata":{},"outputs":[],"source":["<!--Empty Space for separating topics-->\n"]},{"cell_type":"markdown","id":"49e49017-383c-455f-95ba-8acde8113851","metadata":{},"outputs":[],"source":["<h2>About the Authors:</h2> \n","\n","<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD. \n"]},{"cell_type":"markdown","id":"6f64471b-9540-461a-8cc5-bdb274cee01b","metadata":{},"outputs":[],"source":["Other contributors: <a href=\"https://www.linkedin.com/in/michelleccarey/\">Michelle Carey</a>, <a href=\"www.linkedin.com/in/jiahui-mavis-zhou-a4537814a\">Mavis Zhou</a>\n"]},{"cell_type":"markdown","id":"c0f8eee5-e0c6-43ab-854b-4780e0798ee3","metadata":{},"outputs":[],"source":["## Change Log\n","\n","| Date (YYYY-MM-DD) | Version | Changed By | Change Description                                          |\n","| ----------------- | ------- | ---------- | ----------------------------------------------------------- |\n","| 2020-09-23        | 2.0     | Shubham    | Migrated Lab to Markdown and added to course repo in GitLab |\n"]},{"cell_type":"markdown","id":"452a665c-3cef-467c-b791-32193dbd518d","metadata":{},"outputs":[],"source":["<hr>\n"]},{"cell_type":"markdown","id":"81e312a5-9de3-407e-9086-59741a384bfc","metadata":{},"outputs":[],"source":["## <h3 align=\"center\"> Â© IBM Corporation 2020. All rights reserved. <h3/>\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":4}