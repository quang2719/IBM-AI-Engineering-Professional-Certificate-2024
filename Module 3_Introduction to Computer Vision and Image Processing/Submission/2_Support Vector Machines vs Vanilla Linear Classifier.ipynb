{"cells":[{"cell_type":"markdown","id":"9c22c9cc-159d-442c-ab54-532ac6fbf069","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkCV0101ENCoursera872-2023-01-01\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","id":"d90bfd39-8510-4da7-89aa-ea30a97f1600","metadata":{},"outputs":[],"source":["# Support Vector Machine vs Vanilla Linear Classifier\n"]},{"cell_type":"markdown","id":"8fdba9b6-3004-43d2-8c30-f502211b6216","metadata":{},"outputs":[],"source":["<h2>Table of Contents</h2>\n","<p>We will be classifying the popular handwritten data set which we can find in the sklearn library and comparing the results of the logistic regression and SVM.  In the Sklearn library, there are several ways to use logistic regression for multiclass applications; in this lab, we will use  the `multinomial` option; this is like Softmax function we discussed before</p>\n","\n","<ul>\n","    <li>Plotting an Image</li>\n","    <li>Preprocess data for Logistic Regression</li>\n","    <li>Logistic Regression with SkLearn</li>\n","    <li>SVM for Image Classification with SkLearn</li>\n","</ul>\n","<p>Estimated Time Needed: <strong>60 min</strong></p>\n","\n","<hr>\n"]},{"cell_type":"markdown","id":"7b2571e7-1f88-4b1b-a91d-14238afdbf23","metadata":{},"outputs":[],"source":["## Load Important Libraries and Digit Dataset\n"]},{"cell_type":"code","id":"af0953ae-de00-444a-b131-3eaa850d99bb","metadata":{},"outputs":[],"source":["!pip3 install torch torchvision torchaudio"]},{"cell_type":"code","id":"2c313b17-be27-4cc5-bb04-1edf2bcdd4a3","metadata":{},"outputs":[],"source":["import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets, svm, metrics, model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score"]},{"cell_type":"code","id":"9eee55b6-a804-4439-9234-93bb33376f35","metadata":{},"outputs":[],"source":["digits = datasets.load_digits()"]},{"cell_type":"code","id":"7eac0ae1-5511-4676-aa37-ce661f93f76d","metadata":{},"outputs":[],"source":["target = digits.target\nflatten_digits = digits.images.reshape((len(digits.images), -1))"]},{"cell_type":"markdown","id":"9fdfba18-6023-4a00-bab4-4c109bee9443","metadata":{},"outputs":[],"source":["## Visualize Some Handwritten Images in the Dataset\n"]},{"cell_type":"code","id":"1ff0596a-7447-4f10-9cab-cccd80e80e43","metadata":{},"outputs":[],"source":["_, axes = plt.subplots(nrows=1, ncols=5, figsize=(10, 4))\nfor ax, image, label in zip(axes, digits.images, target):\n    ax.set_axis_off()\n    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n    ax.set_title('%i' % label)"]},{"cell_type":"markdown","id":"c77ef809-bc98-4d33-ae1a-78a0f1e4f293","metadata":{},"outputs":[],"source":["## Divide Images into Training and Test Set\n"]},{"cell_type":"markdown","id":"a683c525-60d9-41a8-9766-0bd698dc0fd1","metadata":{},"outputs":[],"source":["I have set the test size to 20% of the total dataset\n"]},{"cell_type":"code","id":"c59b16ab-2ffa-422f-8e82-6cced7550f48","metadata":{},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(flatten_digits, target, test_size=0.2)"]},{"cell_type":"markdown","id":"10206052-090e-4e28-af63-413b2409bf49","metadata":{},"outputs":[],"source":["## Hand-written classification with Logistic Regression\n"]},{"cell_type":"markdown","id":"a8dcdc14-12d8-4af9-b41b-33ce8049553d","metadata":{},"outputs":[],"source":["Standardize the dataset to put all the features of the variables on the same scale\n"]},{"cell_type":"code","id":"21e118d6-959f-4d7a-86a8-8ac2c24fc2a8","metadata":{},"outputs":[],"source":["scaler = StandardScaler()\nX_train_logistic = scaler.fit_transform(X_train)\nX_test_logistic = scaler.transform(X_test)"]},{"cell_type":"markdown","id":"fed334b6-3792-468a-978e-a2761c41ad88","metadata":{},"outputs":[],"source":["Create the logistic regression and fit the logistic regression and use the <code>l1</code> penalty. Note here that since this is a multiclass problem the Logistic Regression parameter `multi_class` is set to `multinomial`.\n"]},{"cell_type":"code","id":"6275589e-5741-4e73-86e8-df4f45508c3d","metadata":{},"outputs":[],"source":["logit = LogisticRegression(C=0.01, penalty='l1', solver='saga', tol=0.1, multi_class='multinomial')"]},{"cell_type":"code","id":"2950a941-a182-4bf2-be44-ec94b477302c","metadata":{},"outputs":[],"source":["logit.fit(X_train_logistic, y_train)"]},{"cell_type":"code","id":"fda80d4e-756c-498e-8e66-841e17a83d75","metadata":{},"outputs":[],"source":["y_pred_logistic = logit.predict(X_test_logistic)"]},{"cell_type":"markdown","id":"db8835a6-399f-4dd7-8a91-b78ecbe6c5fb","metadata":{},"outputs":[],"source":["Get the accuracy of the logistic regression\n"]},{"cell_type":"code","id":"de9fca70-44a8-4298-8ef0-39f81347b2a7","metadata":{},"outputs":[],"source":["print(\"Accuracy: \"+str(logit.score(X_test_logistic, y_test)))"]},{"cell_type":"markdown","id":"c06a0b9a-f01f-4e5f-9fac-06c8abf5bc8c","metadata":{},"outputs":[],"source":["Lets plot out the confusion matrix, each row of the matrix represents the instances in a predicted class, while each column represents the instances in an actual class.\n"]},{"cell_type":"code","id":"c351bc93-531f-4dfc-bc0d-fc5ba8d1ec42","metadata":{},"outputs":[],"source":["label_names = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\ncmx = confusion_matrix(y_test, y_pred_logistic, labels=label_names)"]},{"cell_type":"markdown","id":"0df41ebb-281f-47a2-9a98-688dbcb0598c","metadata":{},"outputs":[],"source":["Accuracy is fine and above 80% but we can see some heavily misclassified values, The classifier had a hard time classifying <code>8</code>\n"]},{"cell_type":"code","id":"45c31a9e-a54e-4109-81aa-005b3843792a","metadata":{},"outputs":[],"source":["df_cm = pd.DataFrame(cmx)\n# plt.figure(figsize=(10,7))\nsns.set(font_scale=1.4) # for label size\nsns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\ntitle = \"Confusion Matrix for SVM results\"\nplt.title(title)\nplt.show()"]},{"cell_type":"markdown","id":"e383cfba-4270-4c1b-ae22-49693e62ff9c","metadata":{},"outputs":[],"source":["## Hand-Written Classification with SVM\n"]},{"cell_type":"markdown","id":"49bd4fac-9255-4c18-a2c1-00b357a9df01","metadata":{},"outputs":[],"source":["Create and fit the SVM model\n"]},{"cell_type":"code","id":"1269370e-7508-4dfb-bc31-e3cc86886ff0","metadata":{},"outputs":[],"source":["svm_classifier = svm.SVC(gamma='scale')"]},{"cell_type":"code","id":"de53e778-7add-4842-a8ba-ff4fd532adf9","metadata":{},"outputs":[],"source":["svm_classifier.fit(X_train, y_train)"]},{"cell_type":"markdown","id":"2722a2e1-ef13-4570-ad96-78172d8e98e9","metadata":{},"outputs":[],"source":["Predict for our test set\n"]},{"cell_type":"code","id":"baaba983-3d8b-4b27-a2eb-01358399e8ce","metadata":{},"outputs":[],"source":["y_pred_svm = svm_classifier.predict(X_test)"]},{"cell_type":"markdown","id":"2d71fd7a-4df0-4d54-8c53-c6d978160b7f","metadata":{},"outputs":[],"source":["Get accuracy for the SVM model, we can see we have a nearly perfect model\n"]},{"cell_type":"code","id":"5bc50d2b-92d7-4e08-83da-b6ae519c0ea1","metadata":{},"outputs":[],"source":["print(\"Accuracy: \"+str(accuracy_score(y_test, y_pred_svm)))"]},{"cell_type":"markdown","id":"a7909cbf-637b-4224-b8f5-8ab2dee42ad1","metadata":{},"outputs":[],"source":["Let's take a look at the confusion matrix for SVM, we can see a nearly perfect model with SVM\n"]},{"cell_type":"code","id":"60cbfa05-3afd-42e0-bf61-76d4e7353878","metadata":{},"outputs":[],"source":["label_names = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\ncmx = confusion_matrix(y_test, y_pred_svm, labels=label_names)"]},{"cell_type":"code","id":"fc7c1f2a-9d5e-483d-95a8-074f82392240","metadata":{},"outputs":[],"source":["df_cm = pd.DataFrame(cmx)\n# plt.figure(figsize=(10,7))\nsns.set(font_scale=1.4) # for label size\nsns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\ntitle = \"Confusion Matrix for SVM results\"\nplt.title(title)\nplt.show()"]},{"cell_type":"markdown","id":"21403087-bcf4-4115-bd3c-4c2f764a64af","metadata":{},"outputs":[],"source":["## Comparing both SVM and Logistic Regression with K-Fold Cross Validation\n","\n","k-fold Cross validation is used when there are limited samples, the handwritten dataset contains about 1800 samples, this will give an opportunity for all the data to be in the training and test set at different given times. We will add <code>l2</code> regularization to visualize how well they both do against SVM.\n"]},{"cell_type":"code","id":"2185ee6c-690a-4669-998b-eb3a2b6f6c69","metadata":{},"outputs":[],"source":["algorithm = []\nalgorithm.append(('SVM', svm_classifier))\nalgorithm.append(('Logistic_L1', logit))\nalgorithm.append(('Logistic_L2', LogisticRegression(C=0.01, penalty='l2', solver='saga', tol=0.1, multi_class='multinomial')))\n\n\nresults = []\nnames = []\ny = digits.target\nfor name, algo in algorithm:\n    k_fold = model_selection.KFold(n_splits=10, random_state=10)\n    if name == 'SVM':\n        X = flatten_digits\n        cv_results = model_selection.cross_val_score(algo, X, y, cv=k_fold, scoring='accuracy')\n    else:\n        scaler = StandardScaler()\n        X = scaler.fit_transform(flatten_digits)\n        cv_results = model_selection.cross_val_score(algo, X, y, cv=k_fold, scoring='accuracy')\n        \n    results.append(cv_results)\n    names.append(name)"]},{"cell_type":"markdown","id":"3a198556-099b-4c2b-bb1d-c67726f1ced5","metadata":{},"outputs":[],"source":["We plot and we can see that SVM performs better all the time even with k-fold cross validation and it is better than both Logistic regressions on average\n"]},{"cell_type":"code","id":"aad8675a-61c4-4bd1-8620-cc20d5794193","metadata":{},"outputs":[],"source":["fig = plt.figure()\nfig.suptitle('Compare Logistic and SVM results')\nax = fig.add_subplot()\nplt.boxplot(results)\nplt.ylabel('Accuracy')\nax.set_xticklabels(names)\nplt.show()"]},{"cell_type":"markdown","id":"2869a408-bcdf-4d64-8a48-1556e013bf2b","metadata":{},"outputs":[],"source":["## References\n"]},{"cell_type":"markdown","id":"9669bb0b-f4a3-455a-9a37-2c7f1bb0831f","metadata":{},"outputs":[],"source":["1.  [Recognizing Hand-written](https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html?utm_medium=Exinfluencer&utm_source=Nurture&utm_content=000026UJ&utm_term=10006555&utm_id=SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-CV0101EN-Coursera-25797139&utm_email=Email&utm_campaign=PLACEHOLDER)\n","2.  [MNIST classification using multinomial logistic + L1](https://scikit-learn.org/stable/auto_examples/linear_model/plot_sparse_logistic_regression_mnist.html?utm_medium=Exinfluencer&utm_source=Nurture&utm_content=000026UJ&utm_term=10006555&utm_id=SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-CV0101EN-Coursera-25797139&utm_email=Email&utm_campaign=PLACEHOLDER)\n"]},{"cell_type":"markdown","id":"372789f2-ffb5-4b8b-bf32-8e89fc9fc833","metadata":{},"outputs":[],"source":["<h2>Authors</h2>\n"]},{"cell_type":"markdown","id":"7ced58b4-a0bb-4a76-a36f-611bcc2487a4","metadata":{},"outputs":[],"source":[" [Aije Egwaikhide](https://www.linkedin.com/in/aije-egwaikhide/?utm_medium=Exinfluencer&utm_source=Nurture&utm_content=000026UJ&utm_term=10006555&utm_id=SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-CV0101EN-Coursera-25797139&utm_email=Email&utm_campaign=PLACEHOLDER) is a Data Scientist at IBM who holds a degree in Economics and Statistics from the University of Manitoba and a Post-grad in Business Analytics from St. Lawrence College, Kingston. She is currently pursuing her Masters in Management Analytics at Queens University. She is part of the IBM Developer Skills Network group where she brings her real-world experience to the courses she creates.\n"]},{"cell_type":"markdown","id":"0f4d02e6-80e8-4a8b-85e9-7f56b16e1ff2","metadata":{},"outputs":[],"source":["# References\n"]},{"cell_type":"markdown","id":"0c38f889-a558-49f5-a612-ccbc8a95298e","metadata":{},"outputs":[],"source":["[1]  <a href='https://opencv.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkCV0101ENCoursera872-2023-01-01'>Open CV</a>\n"]},{"cell_type":"markdown","id":"a93d9f71-a782-4a97-9c68-ba330d798cda","metadata":{},"outputs":[],"source":["<h2>Change Log</h2>\n"]},{"cell_type":"markdown","id":"2db7b7f0-1bfd-4505-b1b9-22a8aae9cdd1","metadata":{},"outputs":[],"source":["<table>\n","    <tr>\n","        <th>Date (YYYY-MM-DD)</th>\n","        <th>Version</th>\n","        <th>Changed By</th>\n","        <th>Change Description</th>\n","    </tr>\n","    <tr>\n","        <td>2021-03-30</td>\n","        <td>0.1</td>\n","        <td>Aije</td>\n","        <td>Created original version of the lab</td>\n","    </tr>\n","</table>\n"]},{"cell_type":"markdown","id":"5fe5edb0-5f25-46bf-b0e7-361e5c016ebb","metadata":{},"outputs":[],"source":["Copyright Â© 2020 IBM Corporation. All rights reserved.\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":4}